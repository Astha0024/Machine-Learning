{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt #pyplot is a module in matplotlib library\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwEPNDWySTKm"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "# We are putting features and dependent variable in different datasets\n",
        "# iloc stands for locate indexes. It will take indexes from the data set \n",
        "# iloc[row range, column range]\n",
        "X = dataset.iloc[:, :-1].values\n",
        "# By using -1 above, we took all columns except the last one\n",
        "# -1 means the last column in python but then range include lower bound and excludes upper bound\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsz2yCebe1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bb10e5-5313-47b1-9def-b90760732ae1"
      },
      "source": [
        "print(X) #Matrix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYrOQ43XcJR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5d6faf-2f8a-465b-c443-9972da62f38a"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSHZ1gSIYK-2"
      },
      "source": [
        "It is important to handle missing data in ML. If we have a large data set with only 1% missing data, we could probably just ignore those rows and remove them. But that is not always the case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93k7ipkSexq"
      },
      "source": [
        "# SimpleImputer is a class from sklearn\n",
        "# We first import this class and then create its object (instance)\n",
        "from sklearn.impute import SimpleImputer \n",
        "# Here we are replacing the missing data with average of all other values in that column\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') # This is the object of SimpleImputer class\n",
        "# Here we apply this object on the matrix of features using a method of this class called 'fit', 'transform'\n",
        "imputer.fit(X[:, 1:3]) #Indexing starts with 0 and upper bound is excluded so this just selects Age and Salary columns\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UgLdMS_bjq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb59bf95-71b9-4041-e392-df5eb9b95a6a"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZoeNygevSu"
      },
      "source": [
        "We are using one hot encoding here. This article explains why one hot encoding can be useful in ML models: https://medium.com/analytics-vidhya/what-why-and-when-of-one-hot-encoding-52d25a5d3aba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hwuVddlSwVi"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Below: [0] means the index that we are applying the transformation on\n",
        "# Below: remainder tells what we want to do with the remaining columns on which we are not applying any transformation\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7QspewyeBfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5d4914-808d-4047-eb63-5dec3e7936b0"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHCShVyTOYY"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhY8-gPpFCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724857ad-d20a-46e1-b341-f53a34daa0c9"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXgA6CzlqbCl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuwQhFdKrYTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042d663f-5ac8-4283-ee58-f8fa529154a8"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUrX_Tvcrbi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e51bf7-db9a-43e2-ab18-e945c7a37cbb"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSMHiIsWreQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33382811-928f-4381-b5d0-ec90d2989d53"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_tW7H56rgtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1426ca-ca08-4211-cca0-17ba923d6a2d"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TyXmE6rj7aP"
      },
      "source": [
        "Feature scaling is done after splitting the data set into test and train. Feature scaling gets mean and SD of features for scaling. This is why we do it after splitting training and test set so that test set is completely new. This avoids data leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM39PI6NdwFx"
      },
      "source": [
        "We are using standardisation for feature scaling here. \n",
        "\n",
        "```\n",
        "[value - mean(features) ] / [ standard deviation (features) ]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxjSUXFQqo-3"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# scaler is fitted only to X_train. We get mean and standard deviation of X_train and then apply the same formula to transform X_train and X_test\n",
        "# We should not get mean and standard deviation of test set as that should technically be unknown in real life scenario\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWPET8ZdlMnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbe5c6f-cb16-4df3-ffd4-12e06ee50029"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXykB_QlRjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af61539-e1fe-400d-8a59-59af5f26ad3f"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}